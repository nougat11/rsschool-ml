{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of rf_classifier.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "235px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3dT6r-VVnJO"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPBzJ5HwVnJU"
      },
      "source": [
        "Fortunately, with libraries such as Scikit-Learn, it’s now easy to build and use almost any machine learning algorithm. But it’s helpful to have an idea of how a machine learning model works under the hood. This lets us diagnose the model when it’s underperforming or explain how it makes decisions, which is crucial if we want to convince others to trust our models.\n",
        "In this assignment, we’ll look at how to build and use the Decision Tree and the Random Forest in Python. We’ll start by understanding how a single decision tree makes classifications on a simple problem. Then, we’ll work our way to using a random forest on a real-world data science problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAO2qTazVnJV"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY5gLcQWVnJV"
      },
      "source": [
        "The dataset we will use in this assignment is the Sonar dataset.\n",
        "\n",
        "This is a dataset that describes sonar chirp returns bouncing off different surfaces. The 60 predictors are the strength of the returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders. There are 208 observations.\n",
        "\n",
        "It is a well-understood dataset. All of the variables are continuous and generally in the range of 0 to 1. The output variable is a string “M” for mine and “R” for rock, which will need to be converted to integers 1 and 0.\n",
        "\n",
        "By predicting the class with the most observations in the dataset (M or mines) the Zero Rule Algorithm can achieve an accuracy of 53%.\n",
        "\n",
        "You can learn more about this dataset at the UCI Machine Learning repository.\n",
        "https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)\n",
        "\n",
        "Download the dataset for free and place it in the \"data\" folder in your working directory with the filename sonar.all-data.csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc2XZzjsVnJW"
      },
      "source": [
        "# Import section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJFXMAUdVnJW"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "np.random.seed(2020)\n",
        "random.seed(2020)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9srnVq7VnJX"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8jK_NAhVnJX"
      },
      "source": [
        "Read data and convert targets to integers 1 and 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "k4nlSYjeVnJX",
        "outputId": "f1bc2131-6e7b-4b39-893c-478e96bdc542"
      },
      "source": [
        "PATH = 'data/'\n",
        "df = pd.read_csv(PATH+'sonar-all-data.csv', header=None)\n",
        "df.columns = [f'feat_{col}' if col!=60 else 'target' for col in df.columns]\n",
        "df['target'] = df['target'].map({'M': 1, 'R': 0})\n",
        "df.head()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feat_0</th>\n",
              "      <th>feat_1</th>\n",
              "      <th>feat_2</th>\n",
              "      <th>feat_3</th>\n",
              "      <th>feat_4</th>\n",
              "      <th>feat_5</th>\n",
              "      <th>feat_6</th>\n",
              "      <th>feat_7</th>\n",
              "      <th>feat_8</th>\n",
              "      <th>feat_9</th>\n",
              "      <th>feat_10</th>\n",
              "      <th>feat_11</th>\n",
              "      <th>feat_12</th>\n",
              "      <th>feat_13</th>\n",
              "      <th>feat_14</th>\n",
              "      <th>feat_15</th>\n",
              "      <th>feat_16</th>\n",
              "      <th>feat_17</th>\n",
              "      <th>feat_18</th>\n",
              "      <th>feat_19</th>\n",
              "      <th>feat_20</th>\n",
              "      <th>feat_21</th>\n",
              "      <th>feat_22</th>\n",
              "      <th>feat_23</th>\n",
              "      <th>feat_24</th>\n",
              "      <th>feat_25</th>\n",
              "      <th>feat_26</th>\n",
              "      <th>feat_27</th>\n",
              "      <th>feat_28</th>\n",
              "      <th>feat_29</th>\n",
              "      <th>feat_30</th>\n",
              "      <th>feat_31</th>\n",
              "      <th>feat_32</th>\n",
              "      <th>feat_33</th>\n",
              "      <th>feat_34</th>\n",
              "      <th>feat_35</th>\n",
              "      <th>feat_36</th>\n",
              "      <th>feat_37</th>\n",
              "      <th>feat_38</th>\n",
              "      <th>feat_39</th>\n",
              "      <th>feat_40</th>\n",
              "      <th>feat_41</th>\n",
              "      <th>feat_42</th>\n",
              "      <th>feat_43</th>\n",
              "      <th>feat_44</th>\n",
              "      <th>feat_45</th>\n",
              "      <th>feat_46</th>\n",
              "      <th>feat_47</th>\n",
              "      <th>feat_48</th>\n",
              "      <th>feat_49</th>\n",
              "      <th>feat_50</th>\n",
              "      <th>feat_51</th>\n",
              "      <th>feat_52</th>\n",
              "      <th>feat_53</th>\n",
              "      <th>feat_54</th>\n",
              "      <th>feat_55</th>\n",
              "      <th>feat_56</th>\n",
              "      <th>feat_57</th>\n",
              "      <th>feat_58</th>\n",
              "      <th>feat_59</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.1609</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.2238</td>\n",
              "      <td>0.0645</td>\n",
              "      <td>0.0660</td>\n",
              "      <td>0.2273</td>\n",
              "      <td>0.3100</td>\n",
              "      <td>0.2999</td>\n",
              "      <td>0.5078</td>\n",
              "      <td>0.4797</td>\n",
              "      <td>0.5783</td>\n",
              "      <td>0.5071</td>\n",
              "      <td>0.4328</td>\n",
              "      <td>0.5550</td>\n",
              "      <td>0.6711</td>\n",
              "      <td>0.6415</td>\n",
              "      <td>0.7104</td>\n",
              "      <td>0.8080</td>\n",
              "      <td>0.6791</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>0.1307</td>\n",
              "      <td>0.2604</td>\n",
              "      <td>0.5121</td>\n",
              "      <td>0.7547</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>0.6692</td>\n",
              "      <td>0.6097</td>\n",
              "      <td>0.4943</td>\n",
              "      <td>0.2744</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>0.2834</td>\n",
              "      <td>0.2825</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.2641</td>\n",
              "      <td>0.1386</td>\n",
              "      <td>0.1051</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>0.0383</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feat_0  feat_1  feat_2  feat_3  ...  feat_57  feat_58  feat_59  target\n",
              "0  0.0200  0.0371  0.0428  0.0207  ...   0.0084   0.0090   0.0032       0\n",
              "1  0.0453  0.0523  0.0843  0.0689  ...   0.0049   0.0052   0.0044       0\n",
              "2  0.0262  0.0582  0.1099  0.1083  ...   0.0164   0.0095   0.0078       0\n",
              "3  0.0100  0.0171  0.0623  0.0205  ...   0.0044   0.0040   0.0117       0\n",
              "4  0.0762  0.0666  0.0481  0.0394  ...   0.0048   0.0107   0.0094       0\n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srLqkotBVnJY"
      },
      "source": [
        "# Split data (train and test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m0doB86VnJZ"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='target'), df['target'], test_size=0.2, random_state=2020)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgCnY2USVnJZ"
      },
      "source": [
        "# Cost functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-490pJ8VnJZ"
      },
      "source": [
        "In this section you should implement two cost functions. Any of these can be used in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfxhpnrWVnJa"
      },
      "source": [
        "## Gini index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDKKVemBVnJa"
      },
      "source": [
        "def gini_index(x):\n",
        "    \"\"\" Calculate Gini Index for a node\n",
        "    Args:\n",
        "        x: Numpy-array of targets in a node\n",
        "    Returns:\n",
        "        float: Gini index\n",
        "    \"\"\"    \n",
        "    if len(x) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    p = np.bincount(x) / len(x)\n",
        "  \n",
        "    return 1 - np.sum(p*p)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TLOCVuKVnJa",
        "outputId": "5af4392f-afc9-4811-97ce-23db11cdf7cd"
      },
      "source": [
        "target = df['target'].values\n",
        "gini_index(target)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4977348372781065"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlaBlsLRVnJb"
      },
      "source": [
        "def gini_gain(parent_node, splits):\n",
        "    \"\"\" Calculate Gini Gain for a particular split\n",
        "    Args:\n",
        "        parent_node: Numpy-array of targets in a parent node\n",
        "        splits: List of two numpy-arrays. Each numpy-array is targets in a child node\n",
        "    Returns:\n",
        "        float: Gini gain\n",
        "    \"\"\"       \n",
        "    splits_gini = np.sum([gini_index(split)*(len(split)/len(parent_node)) for split in splits])\n",
        "    return gini_index(parent_node) - splits_gini"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I901iQtVnJb",
        "outputId": "6f70c07f-eb4b-49bb-8ad5-cf21fc0df196"
      },
      "source": [
        "splits = [np.random.choice(df['target'].values, 100), np.random.choice(df['target'].values, 108)]\n",
        "gini_gain(target, splits)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.008728427021696183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-Cdlg1bVnJc"
      },
      "source": [
        "## Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyGDmgPbVnJc"
      },
      "source": [
        "def entropy(x):\n",
        "    \"\"\" Calculate Entropy for a node\n",
        "    Args:\n",
        "        x: Numpy-array of targets in a node\n",
        "    Returns:\n",
        "        float: Entropy\n",
        "    \"\"\"\n",
        "    if len(x) == 0:\n",
        "        return 0.0\n",
        "  \n",
        "    p = np.clip(np.bincount(x) / len(x), 1e-15, 1.)\n",
        "    \n",
        "    return -np.sum(p * np.log(p))"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnCMMYWiVnJc"
      },
      "source": [
        "def information_gain(parent_node, splits):\n",
        "    \"\"\" Calculate Information Gain for a particular split\n",
        "    Args:\n",
        "        parent_node: Numpy-array of targets in a parent node\n",
        "        splits: List of two numpy-arrays. Each numpy-array is targets in a child node\n",
        "    Returns:\n",
        "        float: Information Gain\n",
        "    \"\"\"     \n",
        "    splits_entropy = np.sum([entropy(split)*(len(split)/len(parent_node)) for split in splits])\n",
        "    print(splits_entropy)\n",
        "    return entropy(parent_node) - splits_entropy"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgmnoMfRXw4x",
        "outputId": "4ec019de-2a8b-4928-d061-b1fdc03e25bd"
      },
      "source": [
        "import numpy as np\r\n",
        "splits = [np.array([1,1,0,0,0,0,0,0,0]), np.array([1,1,1,1,1,1,1,1,1,0,0])]\r\n",
        "parent_node=np.array([1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0])\r\n",
        "print(len(parent_node))\r\n",
        "information_gain(parent_node, splits)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "0.4991444117577551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18899440195583334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Z3PRiLVnJc"
      },
      "source": [
        "# Split function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHxiNOZBVnJc"
      },
      "source": [
        "Implement split functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp_uPz9pVnJd"
      },
      "source": [
        "def split(X, y, value):\n",
        "    \"\"\" Split y-values in order to calculate gain later\n",
        "    Args:\n",
        "        X: 1-dimensional numpy-array of data predictor with shape (N,)\n",
        "        y: 1-dimensional numpy-array of targets with shape (N,)\n",
        "        value (float): the value by which the X should be splitted\n",
        "    Returns:\n",
        "        Two 1-dimensional numpy-arrays with targets related to splits\n",
        "    \"\"\"      \n",
        "    left_mask = X < value\n",
        "    right_mask = X >= value\n",
        "    return y[left_mask], y[right_mask]\n",
        "\n",
        "\n",
        "def split_dataset(X, y, column, value):\n",
        "    \"\"\" Split dataset by a particular column and value\n",
        "    Args:\n",
        "        X: 2-dimensional numpy-array (N, num_feats). N-number of samples\n",
        "        y: 1-dimensional numpy-array of targets with shape (N,)  \n",
        "        column (int): the column by which the X should be splitted\n",
        "        value (float): the value by which the column should be splitted\n",
        "    Returns:\n",
        "        Two 2-dimensional numpy-arrays with data and two 1-dimensional numpy-arrays with targets related to splits\n",
        "        left_X, right_X, left_y, right_y\n",
        "    \"\"\"       \n",
        "    left_mask = X[:, column] < value\n",
        "    right_mask = X[:, column] >= value\n",
        "    left_y, right_y = y[left_mask], y[right_mask]\n",
        "    left_X, right_X = X[left_mask], X[right_mask]\n",
        "    return left_X, right_X, left_y, right_y"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FthrDXhbVnJd"
      },
      "source": [
        "# Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAWy4gklVnJd"
      },
      "source": [
        "class Tree(object):\n",
        "    \"\"\"A decision tree classifier.\n",
        "\n",
        "    Args:\n",
        "        criterion : {\"gini_gain\", \"information_gain\"}\n",
        "    \"\"\"\n",
        "    def __init__(self, criterion=None):\n",
        "        self.impurity = None\n",
        "        self.threshold = None\n",
        "        self.column_index = None\n",
        "        self.outcome_probs = None\n",
        "        self.criterion = criterion\n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "\n",
        "    @property\n",
        "    def is_terminal(self):\n",
        "        \"\"\" Define is it terminal node\n",
        "        \"\"\"          \n",
        "        return not bool(self.left_child and self.right_child)\n",
        "\n",
        "    def _find_splits(self, X):\n",
        "        \"\"\"Find all possible split values.\"\"\"\n",
        "        split_values = set()\n",
        "\n",
        "        # Get unique values in a sorted order\n",
        "        x_unique = list(np.unique(X))\n",
        "        for i in range(1, len(x_unique)):\n",
        "            # Find a point between two values\n",
        "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\n",
        "            split_values.add(average)\n",
        "\n",
        "        return list(split_values)\n",
        "\n",
        "    def _find_best_split(self, X, y, n_features):\n",
        "        \"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\n",
        "\n",
        "        # Sample random subset of features\n",
        "        subset = random.sample(list(range(0, X.shape[1])), n_features)\n",
        "        max_gain, max_col, max_val = None, None, None\n",
        "\n",
        "        for column in subset:\n",
        "            split_values = self._find_splits(X[:, column])\n",
        "\n",
        "            for value in split_values:\n",
        "                splits = split(X[:, column], y, value)\n",
        "                \n",
        "                gain = self.criterion(y, splits)\n",
        "\n",
        "                if (max_gain is None) or (gain > max_gain):\n",
        "                    max_col, max_val, max_gain = column, value, gain\n",
        "        return max_col, max_val, max_gain\n",
        "\n",
        "    def fit(self, X, y, n_features=None, max_depth=None):\n",
        "        \"\"\"Fit model.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The training input samples. 2-dimensional numpy array.\n",
        "            y (numpy-array): The target values. 1-dimensional numpy array.\n",
        "            n_features (int): The number of features when fit is performed (default: all features)\n",
        "            max_depth (int): The maximum depth of the tree. If None, then nodes are expanded until\n",
        "                             all leaves are pure.\n",
        "        \"\"\"        \n",
        "        try:\n",
        "            # Exit from recursion using assert syntax\n",
        "            if max_depth is not None:\n",
        "                assert max_depth > 0\n",
        "                max_depth -= 1\n",
        "\n",
        "            if n_features is None:\n",
        "                n_features = X.shape[1]\n",
        "\n",
        "            column, value, gain = self._find_best_split(X, y, n_features)\n",
        "            assert gain is not None\n",
        "\n",
        "            self.column_index = column\n",
        "            self.threshold = value\n",
        "            self.impurity = gain\n",
        "\n",
        "            # Split dataset\n",
        "            left_X, right_X, left_target, right_target = split_dataset(X, y, column, value)\n",
        "\n",
        "            # Grow left and right child\n",
        "            self.left_child = Tree(self.criterion)\n",
        "            self.left_child.fit(\n",
        "                left_X, left_target, n_features, max_depth\n",
        "            )\n",
        "\n",
        "            self.right_child = Tree(self.criterion)\n",
        "            self.right_child.fit(\n",
        "                right_X, right_target, n_features, max_depth\n",
        "            )\n",
        "        except AssertionError:\n",
        "            self.outcome_probs = np.around(np.sum(y) / y.shape[0])\n",
        "\n",
        "\n",
        "    def predict_row(self, row):\n",
        "        \"\"\"Predict single row.\"\"\"\n",
        "        if not self.is_terminal:\n",
        "            if row[self.column_index] < self.threshold:\n",
        "                return self.left_child.predict_row(row)\n",
        "            else:\n",
        "                return self.right_child.predict_row(row)\n",
        "        print(self.outcome_probs)\n",
        "        return self.outcome_probs\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The test input samples. 2-dimensional numpy array.\n",
        "        \"\"\"  \n",
        "        result = np.zeros(X.shape[0])\n",
        "        for i in range(X.shape[0]):\n",
        "            result[i] = self.predict_row(X[i, :])\n",
        "        return result"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqo24opQVnJi"
      },
      "source": [
        "Fit two models with \"max_depth=3\" and \"max_depth=None\" hyperparameters. Explain the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnNkyz2iVnJl",
        "outputId": "7efdc55d-413f-40f6-bc5f-ec91a7018fba"
      },
      "source": [
        "model = Tree(criterion=gini_gain)\n",
        "model.fit(X_train.values, y_train.values)\n",
        "y_pred = model.predict(X_test.values)\n",
        "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "0.0\n",
            "1.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "Accuracy score is: 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JIzFaxcVnJl"
      },
      "source": [
        "model = Tree(criterion=gini_gain)\n",
        "model.fit(X_train.values, y_train.values, max_depth=3)\n",
        "y_pred = model.predict(X_test.values)\n",
        "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P6kpVXSVnJm"
      },
      "source": [
        "model = Tree(criterion=information_gain)\n",
        "model.fit(X_train.values, y_train.values, max_depth=3)\n",
        "y_pred = model.predict(X_test.values)\n",
        "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7AsRHyHVnJm"
      },
      "source": [
        "# Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY0SJj2zVnJm"
      },
      "source": [
        "class RandomForestClassifier(object):\n",
        "    \"\"\"\n",
        "    A random forest classifier.\n",
        "    A random forest is a meta estimator that fits a number of decision tree\n",
        "    classifiers on various sub-samples of the dataset and uses averaging to\n",
        "    improve the predictive accuracy and control overfitting.\n",
        "    \n",
        "    Args:\n",
        "        n_estimators : int, default=10\n",
        "            The number of trees in the forest.\n",
        "\n",
        "        max_depth : int, default=None\n",
        "            The maximum depth of the tree. If None, then nodes are expanded until\n",
        "            all leaves are pure.        \n",
        "\n",
        "        n_features : int, default=None\n",
        "            The number of features to consider when looking for the best split.\n",
        "            If None, then `n_features=sqrt(n_features)`.\n",
        "\n",
        "        criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
        "            The function to measure the quality of a split. Supported criteria are\n",
        "            \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_estimators=10, max_depth=None, n_features=None, criterion=\"entropy\", bootstrap=True):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.n_features = n_features\n",
        "        self.bootstrap = bootstrap\n",
        "        \n",
        "        if criterion == \"entropy\":\n",
        "            self.criterion = information_gain\n",
        "        elif criterion == \"gini\":\n",
        "            self.criterion = gini_gain\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown criterion '{criterion}'\")\n",
        "            \n",
        "        self.trees = [Tree(criterion=self.criterion) for _ in range(n_estimators)]\n",
        "        \n",
        "    def _init_data(self, X, y):\n",
        "        \"\"\"Ensure data are in the expected format.\n",
        "        Ensures X and y are stored as numpy ndarrays by converting from an\n",
        "        array-like object if necessary. \n",
        "        Parameters\n",
        "        Args:\n",
        "            X : array-like\n",
        "                Feature dataset.\n",
        "            y : array-like, default=None\n",
        "                Target values. By default is required, but if y_required = false\n",
        "                then may be omitted.\n",
        "        \"\"\"\n",
        "        self.size = len(X)\n",
        "        \n",
        "        if not isinstance(X, np.ndarray):\n",
        "            self.X = np.array(X)\n",
        "        else:\n",
        "            self.X = X\n",
        "\n",
        "        if not isinstance(y, np.ndarray):\n",
        "            self.y = np.array(y)\n",
        "        else:\n",
        "            self.y = y\n",
        "            \n",
        "    def bootstrap_data(self, size):\n",
        "        return np.random.randint(size, size=size)\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit model.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The training input samples. 2-dimensional numpy array.\n",
        "            y (numpy-array): The target values. 1-dimensional numpy array.\n",
        "        \"\"\"         \n",
        "        if self.n_features is None:\n",
        "            self.n_features = int(np.sqrt(X.shape[1]))\n",
        "        elif X.shape[1] < self.n_features:\n",
        "            raise ValueError(f\"'n_features should be <= n_features'\")\n",
        "            \n",
        "        self._init_data(X, y)\n",
        "        \n",
        "        for tree in self.trees:\n",
        "            if self.bootstrap:\n",
        "                idxs = self.bootstrap_data(self.size)\n",
        "                X = self.X[idxs]\n",
        "                y = self.y[idxs]\n",
        "            else:\n",
        "                X = self.X\n",
        "                y = self.y\n",
        "                \n",
        "            tree.fit(\n",
        "                X,\n",
        "                y,\n",
        "                n_features=self.n_features,\n",
        "                max_depth=self.max_depth,\n",
        "            )\n",
        "            \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions.\n",
        "\n",
        "        Args:\n",
        "            X (numpy-array): The test data input samples. 2-dimensional numpy array.\n",
        "        \"\"\"            \n",
        "        if not isinstance(X, np.ndarray):\n",
        "            X = np.array(X)\n",
        "\n",
        "        if self.X is not None:\n",
        "            predictions = np.zeros(len(X))\n",
        "            for i in range(len(X)):\n",
        "                row_pred = 0.\n",
        "                for tree in self.trees:\n",
        "                    row_pred += tree.predict_row(X[i, :])\n",
        "\n",
        "                row_pred /= self.n_estimators\n",
        "                predictions[i] = round(row_pred)\n",
        "            return predictions  \n",
        "        else:\n",
        "            raise ValueError(\"You should fit a model before `predict`\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWxOHpkqVnJm"
      },
      "source": [
        "Fit two models with \"n_estimators=10\" and \"n_estimators=100\" hyperparameters. Explain the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2qCZkroVnJn"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=10, max_depth=None, n_features=None, criterion=\"entropy\")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5aFzmumVnJn"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=100, max_depth=None, n_features=None, criterion=\"entropy\")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJsP_SjPVnJn"
      },
      "source": [
        "Now it's your turn to explore the various parameters of sklearn [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and their influence on model quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Y8_SnVVnJn"
      },
      "source": [
        "# Homework part 1. RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9wfFivAVnJn"
      },
      "source": [
        "_Note_: Consider **accuracy** as main metric of model performance on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFG83m79VnJo"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp94FSOBVnJo"
      },
      "source": [
        "**Task 1 (0.5 points)** Split the dataset into train, test and validation parts (0.6 / 0.2 / 0.2). First two will be used for model hyperparameter tuning whereas the best model quality should be evaluated on validation part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJDygXGJVnJo"
      },
      "source": [
        "X = df.drop('target', axis = 1)\r\n",
        "y = df['target']\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2000, random_state = 2021)\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 2021)\r\n",
        "print(f\"Size train: {len(X_train) / len(X)}; Size test {len(X_test)/len(X)}; Size val{len(X_val)/len(X)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7d5-idkiozA"
      },
      "source": [
        "Плюс минус все норм"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53kDTU62VnJo"
      },
      "source": [
        "**Task 2 (2 points)**. Apply '_greedy_' hyperparameter tuning of RandomForestClassifier model. '_Greedy_' way means the following general approach. At first we tune one model parameter whereas others are fixed with default values. Then we move on to tune the second parameter whereas others are fixed default values and first has the best values from previous step. After it we tune the third parameter with best fixed values for previous two and default values for the rest. Repeat until we go through all the parameters, then repeat this cycle if you are seeing a clear increase in the test metric. <br>\n",
        "\n",
        "Although this approach has a lot of disadvantages (you may think which ones), sometimes that is the only way to tune model typerparams due to big training time **if you understand how the model parameters are interrelated and the tuning order takes those dependencies into account.**<br>\n",
        "\n",
        "Here is one of the possible options for RandomForestClassifier:\n",
        "- Choose a decent value for number of trees using '_elbow_' rule. You may plot the dependence of RMSE on trees_num and pick up the number after which the error decreases not **as much as before**. \n",
        "- Pick up the best split criterion ('gini' / 'entropy') and then tune _max_depth_, _min_samples_split_, _min_samples_leaf_.\n",
        "- Increase number of trees with best found parameters so far.\n",
        "- Repeat this excersice starting from picking the best split criterion while other params are fixed with best values from previous steps **if you observe a significant test metric improvement**. Otherwise just stop and measure your best model result on validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWkW2vKEjWDz"
      },
      "source": [
        "Локтем подберем число деревьев"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awRDVik1VnJo"
      },
      "source": [
        "from math import sqrt\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "errors = []\r\n",
        "for i in range(1, 101):\r\n",
        "  model = RandomForestClassifier(random_state= 2021, n_estimators=i)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  Y_pred = model.predict(X_test)\r\n",
        "  errors.append(accuracy_score(y_test, Y_pred))\r\n",
        "X = np.arange(1, 101, 1)\r\n",
        "plt.plot(X, errors)\r\n",
        "plt.show() \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIRrjsfdlvT2"
      },
      "source": [
        "зазумим область![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSU1IJI7l18j"
      },
      "source": [
        "errors = []\r\n",
        "for i in range(14, 25):\r\n",
        "  model = RandomForestClassifier(random_state= 2021, n_estimators=i)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  Y_pred = model.predict(X_test)\r\n",
        "  errors.append(accuracy_score(y_test, Y_pred))\r\n",
        "X = np.arange(14, 25, 1)\r\n",
        "plt.plot(X, errors)\r\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DvWvdWBl-pP"
      },
      "source": [
        "По-видимому количество 15. Найдем лучший критерий"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMKvDOtkmRdM"
      },
      "source": [
        "model = RandomForestClassifier(random_state= 2021, n_estimators=15, criterion='gini')\r\n",
        "model.fit(X_train, y_train)\r\n",
        "Y_pred = model.predict(X_test)\r\n",
        "print(f\"gini accuracy: {accuracy_score(y_test, Y_pred)}\")\r\n",
        "model = RandomForestClassifier(random_state= 2021, n_estimators=15, criterion='entropy')\r\n",
        "model.fit(X_train, y_train)\r\n",
        "Y_pred = model.predict(X_test)\r\n",
        "print(f\"entropy accuracy: {accuracy_score(y_test, Y_pred)}\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG-b97jNmOQL"
      },
      "source": [
        "гини лучше, будем бустить другие параметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGQedFA8n9B_"
      },
      "source": [
        "errors = []\r\n",
        "for i in range(5, 20):\r\n",
        "  model = RandomForestClassifier(random_state= 2021, n_estimators=15, max_depth=i)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  Y_pred = model.predict(X_test)\r\n",
        "  errors.append(accuracy_score(y_test, Y_pred))\r\n",
        "X = np.arange(5, 20, 1)\r\n",
        "plt.plot(X, errors)\r\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNyNSaB4oTnV"
      },
      "source": [
        "max_depth = 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8NKqYd9obXK"
      },
      "source": [
        "errors = []\r\n",
        "for i in range(2, 10):\r\n",
        "  model = RandomForestClassifier(random_state= 2021, n_estimators=15, max_depth=8, min_samples_split=i)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  Y_pred = model.predict(X_test)\r\n",
        "  errors.append(accuracy_score(y_test, Y_pred))\r\n",
        "X = np.arange(2, 10, 1)\r\n",
        "plt.plot(X, errors)\r\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53n0GwFXpM86"
      },
      "source": [
        "min_samples_split = 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zo68kxBphuz"
      },
      "source": [
        "errors = []\r\n",
        "for i in range(2, 15):\r\n",
        "  model = RandomForestClassifier(random_state= 2021, n_estimators=15, max_depth=8, min_samples_split=2, min_samples_leaf=i)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  Y_pred = model.predict(X_test)\r\n",
        "  errors.append(accuracy_score(y_test, Y_pred))\r\n",
        "X = np.arange(2, 15, 1)\r\n",
        "plt.plot(X, errors)\r\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCekmJ3cp6SH"
      },
      "source": [
        "min_samples_leaf = 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hidoxiTHqImv"
      },
      "source": [
        "errors = []\r\n",
        "for i in range(15, 100):\r\n",
        "  model = RandomForestClassifier(random_state= 2021, n_estimators=i, max_depth=8, min_samples_split=2, min_samples_leaf=12)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  Y_pred = model.predict(X_test)\r\n",
        "  errors.append(accuracy_score(y_test, Y_pred))\r\n",
        "X = np.arange(15, 100, 1)\r\n",
        "plt.plot(X, errors)\r\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo_WR_V9qjkj"
      },
      "source": [
        "При увеличении числа деревьев, результаты не улучшились, останавливаемся"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g-alB_2VnJo"
      },
      "source": [
        "### Grid Search and Random Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6fp6WVMVnJp"
      },
      "source": [
        "If you have enough computational power for model training in a reasonable amount of time more sophisticated approach of hyperparameter tuning would be either Grid Search or Random Search.<br>\n",
        "\n",
        "In a nutshell Grid Search allows you to pass through all different combinations of given model parameters and their values and choose the best combination. Whereas Random Search would randomly choose values for given model parameters and evaluate them on test data untill it reaches the specified number of iterations.<br>\n",
        "\n",
        "More information here [Gentle introduction to Grid and Random search](https://medium.com/@senapati.dipak97/grid-search-vs-random-search-d34c92946318) and here [Detailed Explanation with code examples](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IinpNaE9VnJp"
      },
      "source": [
        "![grid_random_search.png](attachment:grid_random_search.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyQ0WeY0VnJp"
      },
      "source": [
        "**Task 3 (1 point)**. Compare your previous results with [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) hyperparameter tuning. You may tune best hyperparameters for forest with several trees and then increase it while measure the quality on validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu-1J5blVnJp",
        "outputId": "7369ed65-2213-4e57-ea53-11a035f4efb5"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "estimators = [1, 2, 5, 10, 20, 25, 50, 100, 200, 250, 500, 1000]\r\n",
        "leafs = list(range(1, 20))\r\n",
        "parameters = {'random_state': [2021], 'n_estimators': estimators, 'criterion': ['gini', 'entropy'], 'min_samples_leaf': leafs}\r\n",
        "model = RandomForestClassifier()\r\n",
        "clf = GridSearchCV(model, parameters, verbose = 10, n_jobs=-1)\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "print(clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 456 candidates, totalling 2280 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1741s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1114s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:    6.4s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (3.0187s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0615s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:   15.5s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0574s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed:   18.0s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.9652s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:   29.5s\n",
            "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed:   29.7s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1947s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed:   32.0s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0621s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 183 tasks      | elapsed:   42.1s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1975s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 198 tasks      | elapsed:   42.6s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.2996s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 225 tasks      | elapsed:   46.9s\n",
            "[Parallel(n_jobs=-1)]: Done 243 tasks      | elapsed:   56.6s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1821s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1306s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done 263 tasks      | elapsed:   57.2s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0149s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 313 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1899s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 336 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0123s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed:  1.4min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1929s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1841s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done 412 tasks      | elapsed:  1.5min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.6768s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1051s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0949s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (3.9209s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 478 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1895s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0194s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 545 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1877s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1996s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done 592 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.5593s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1037s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0928s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (3.8840s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 662 tasks      | elapsed:  2.5min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1901s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 700 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0193s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1921s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 737 tasks      | elapsed:  2.8min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1868s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.4792s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1043s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0838s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done 816 tasks      | elapsed:  3.0min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (3.8680s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1896s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed:  3.2min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.1151s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 914 tasks      | elapsed:  3.5min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1928s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1764s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.4627s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1129s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0925s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done 1000 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (3.8160s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1887s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 1052 tasks      | elapsed:  3.9min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0992s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1843s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 1101 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.2756s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1829s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 1155 tasks      | elapsed:  4.3min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1636s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.3635s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 1227 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1948s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.2859s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 1277 tasks      | elapsed:  4.8min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1958s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.5453s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 1329 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1976s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1569s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.1624s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 1402 tasks      | elapsed:  5.4min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1879s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.1782s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 1457 tasks      | elapsed:  5.6min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1966s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.2062s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 1516 tasks      | elapsed:  5.8min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1977s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.1686s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 1577 tasks      | elapsed:  6.1min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1849s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.1842s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 1638 tasks      | elapsed:  6.3min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1927s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.1169s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1916s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 1701 tasks      | elapsed:  6.6min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.1567s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1932s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done 1766 tasks      | elapsed:  6.8min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.3460s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1829s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1416s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Done 1833 tasks      | elapsed:  7.1min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0436s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1941s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0555s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 1919 tasks      | elapsed:  7.5min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1883s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.3131s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 1981 tasks      | elapsed:  7.8min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1918s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1408s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.0056s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 2064 tasks      | elapsed:  8.0min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1882s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.3416s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1901s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.2657s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 2140 tasks      | elapsed:  8.3min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1813s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1403s.) Setting batch_size=4.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.9821s.) Setting batch_size=1.\n",
            "[Parallel(n_jobs=-1)]: Done 2225 tasks      | elapsed:  8.7min\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1937s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Batch computation too slow (2.3074s.) Setting batch_size=1.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'criterion': 'entropy', 'min_samples_leaf': 1, 'n_estimators': 50, 'random_state': 2021}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 2280 out of 2280 | elapsed:  8.9min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw-88u9MzZmJ",
        "outputId": "078f6158-2114-4869-d63d-bf42d11f0f79"
      },
      "source": [
        "model = RandomForestClassifier(random_state= 2021, n_estimators=50, criterion='entropy', min_samples_leaf=1)\r\n",
        "model.fit(X_train, y_train)\r\n",
        "Y_pred = model.predict(X_val)\r\n",
        "print(f\"GRID accuracy: {accuracy_score(y_val, Y_pred)}\")\r\n",
        "model = RandomForestClassifier(random_state= 2021, n_estimators=15, max_depth=8, min_samples_split=2, min_samples_leaf=12)\r\n",
        "model.fit(X_train, y_train)\r\n",
        "Y_pred = model.predict(X_val)\r\n",
        "print(f\"brute accuracy: {accuracy_score(y_val, Y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRID accuracy: 0.8571428571428571\n",
            "brute accuracy: 0.8095238095238095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcRu9nB33Y9s"
      },
      "source": [
        "Grid оказался лучше"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXcR14vAzTB8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgn8hQc3VnJp"
      },
      "source": [
        "**Task 4 (1 point)**. And finally tune forest hyperparameters with [RandomSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). Compare results to previous attempts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DabQLsFpVnJp",
        "outputId": "167da28a-8f8e-4f09-eaf4-1a5004736399"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "estimators = list(range(1, 1001))\r\n",
        "leafs = list(range(1, 101))\r\n",
        "depth = list(range(2, 101))\r\n",
        "spl = list(range(2, 101))\r\n",
        "parameters = {'random_state': [2021], 'n_estimators': estimators, 'criterion': ['gini', 'entropy'], 'min_samples_leaf': leafs, 'max_depth': depth, 'min_samples_split': spl}\r\n",
        "model = RandomForestClassifier()\r\n",
        "clf = RandomizedSearchCV(model, parameters, verbose = 10, n_jobs=-1)\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "print(clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.2s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    6.5s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   13.5s\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   18.1s\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   24.6s\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   29.4s\n",
            "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   30.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'random_state': 2021, 'n_estimators': 237, 'min_samples_split': 18, 'min_samples_leaf': 4, 'max_depth': 42, 'criterion': 'gini'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfsDeT-P47xc",
        "outputId": "ddefa43f-9d57-4af0-f851-53ae0411e0ab"
      },
      "source": [
        "model = RandomForestClassifier(random_state= 2021, n_estimators=50, criterion='entropy', min_samples_leaf=1)\r\n",
        "model.fit(X_train, y_train)\r\n",
        "Y_pred = model.predict(X_val)\r\n",
        "print(f\"GRID error: {accuracy_score(y_val, Y_pred)}\")\r\n",
        "model = RandomForestClassifier(random_state= 2021, n_estimators=15, max_depth=8, min_samples_split=2, min_samples_leaf=12)\r\n",
        "model.fit(X_train, y_train)\r\n",
        "Y_pred = model.predict(X_val)\r\n",
        "print(f\"brute error: {accuracy_score(y_val, Y_pred)}\")\r\n",
        "model = RandomForestClassifier(random_state= 2021, n_estimators=864, criterion='entropy', min_samples_leaf=2, max_depth=57, min_samples_split = 27)\r\n",
        "model.fit(X_train, y_train)\r\n",
        "Y_pred = model.predict(X_val)\r\n",
        "print(f\"random1 error: {accuracy_score(y_val, Y_pred)}\")\r\n",
        "model = RandomForestClassifier(random_state= 2021, n_estimators=237, criterion='gini', min_samples_leaf=4, max_depth=42, min_samples_split = 18 )\r\n",
        "model.fit(X_train, y_train)\r\n",
        "Y_pred = model.predict(X_val)\r\n",
        "print(f\"random2 error: {accuracy_score(y_val, Y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRID error: 0.8571428571428571\n",
            "brute error: 0.8095238095238095\n",
            "random1 error: 0.8571428571428571\n",
            "random2 error: 0.8333333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTJatuctVnJq"
      },
      "source": [
        "**Task 5 (0.5 points)**. Tell us about your experience in hyperparameter tuning with the approaches above. What do you think would be the best option for this task and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AmoYQx0VnJq"
      },
      "source": [
        "Тюнинг параметров - вещь интересная. В кагле это может привести не к добру. Как видно из результатов выше, грид дает наилучший результат, однако и время затраченное на его слишком высоко. Рандом дает либо наилучший результат, либо что-то странное. Какой метод лучше однозначно сказать нельзя. Все зависит от имеющихся ресурсов. Если есть все время планеты или мощный суперкомпьютер, то явно grid, в других случаях явно несколько попыток рандома. Так же явно влияет подбор параметров для поиска. Как это сделать оптимально?!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7O6cpRfVnJq"
      },
      "source": [
        "### Desicion tree explained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fSlsUiyVnJq"
      },
      "source": [
        "Remember the [Titanic](https://www.kaggle.com/c/titanic) competition from last week? Wouldn't be a good idea to visualize one of possible desicion-making processes of _survived_ / _dead_ labeling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0Q633RyVnJq"
      },
      "source": [
        "**Task 6 (1 point)**. Load titanic dataset, split it into train/test parts, apply simple hyperparameter tuning of [DesicionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) (use one of the approaches above) in order to have **test accuracy more than 0.65**. <br>\n",
        "\n",
        "Draw the best tree decision making process. You may use [sklearn.tree.prot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html).<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOnYUdooVnJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49869636-6f6e-4cf3-d019-9f45a02b6dae"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "df = pd.read_csv(PATH+'train.csv').set_index('PassengerId')\n",
        "\n",
        "model = DecisionTreeClassifier(random_state = 2021)\n",
        "X = df.drop('Survived', axis = 1)\n",
        "y = df['Survived']\n",
        "X = X.fillna(method='ffill')\n",
        "X = pd.get_dummies(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 2021)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7690582959641256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "hWrCTskIkfVF",
        "outputId": "fc852494-3f76-47b1-d97c-efe0fd668411"
      },
      "source": [
        "from IPython.display import Image\r\n",
        "Image(url='https://media.giphy.com/media/LVIbmaUwpRJ1C/giphy.gif')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://media.giphy.com/media/LVIbmaUwpRJ1C/giphy.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcm0gxmgko1v"
      },
      "source": [
        "и тюнить не пришлось"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLNXFm9DVnJq"
      },
      "source": [
        "Is it easy to interpret its results? Are you able to explain to a random person why would he survive / die on the titanic?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8xAzgLBVnJr"
      },
      "source": [
        "# Homework part 2. RandomForestRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oef0V0mJVnJr"
      },
      "source": [
        "**Task 7 (2 points)**. Write your own *DecisionTreeRegressor* class with _MSE_ split criterion and settable parameter *max_depth*. Demonstrate its consistency on the proposed artificial data (or some other) by comparing MSE of train predictions with [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). <br>\n",
        "\n",
        "Provide examples for different _max_depth_ parameter.\n",
        "\n",
        "Of course you may re-use code for *DecisionTreeClassifier*. You need to figure out what needs to be changed in it for Classification -> Regression transformation.<br>\n",
        "\n",
        "**! You are allowed to use only NumPy library** in this assigment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWykafcH5RFC"
      },
      "source": [
        "class Tree_regr(object):\r\n",
        "    \"\"\"A decision tree classifier.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        criterion : {\"gini_gain\", \"information_gain\"}\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, criterion=None):\r\n",
        "        self.impurity = None\r\n",
        "        self.threshold = None\r\n",
        "        self.column_index = None\r\n",
        "        self.outcome_probs = None\r\n",
        "        self.criterion = criterion\r\n",
        "        self.left_child = None\r\n",
        "        self.right_child = None\r\n",
        "        \r\n",
        "\r\n",
        "    @property\r\n",
        "    def is_terminal(self):\r\n",
        "        \"\"\" Define is it terminal node\r\n",
        "        \"\"\"          \r\n",
        "        return not bool(self.left_child and self.right_child)\r\n",
        "\r\n",
        "    def _find_splits(self, X):\r\n",
        "        \"\"\"Find all possible split values.\"\"\"\r\n",
        "        split_values = set()\r\n",
        "\r\n",
        "        # Get unique values in a sorted order\r\n",
        "        x_unique = list(np.unique(X))\r\n",
        "        for i in range(1, len(x_unique)):\r\n",
        "            # Find a point between two values\r\n",
        "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\r\n",
        "            split_values.add(average)\r\n",
        "\r\n",
        "        return list(split_values)\r\n",
        "\r\n",
        "    def _find_best_split(self, X, y, n_features):\r\n",
        "        \"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\r\n",
        "\r\n",
        "        # Sample random subset of features\r\n",
        "        subset = random.sample(list(range(0, X.shape[1])), n_features)\r\n",
        "        max_gain, max_col, max_val = None, None, None\r\n",
        "\r\n",
        "        for column in subset:\r\n",
        "            split_values = self._find_splits(X[:, column])\r\n",
        "\r\n",
        "            for value in split_values:\r\n",
        "                splits = split(X[:, column], y, value)\r\n",
        "                \r\n",
        "                gain = np.mean(y)\r\n",
        "\r\n",
        "                if (max_gain is None) or (gain > max_gain):\r\n",
        "                  max_col, max_val, max_gain = column, value, gain\r\n",
        "        return max_col, max_val, max_gain\r\n",
        "\r\n",
        "    def fit(self, X, y, n_features=None, max_depth=None):\r\n",
        "        \"\"\"Fit model.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            X (numpy-array): The training input samples. 2-dimensional numpy array.\r\n",
        "            y (numpy-array): The target values. 1-dimensional numpy array.\r\n",
        "            n_features (int): The number of features when fit is performed (default: all features)\r\n",
        "            max_depth (int): The maximum depth of the tree. If None, then nodes are expanded until\r\n",
        "                             all leaves are pure.\r\n",
        "        \"\"\"        \r\n",
        "        try:\r\n",
        "            # Exit from recursion using assert syntax\r\n",
        "            if max_depth is not None:\r\n",
        "                assert max_depth > 0\r\n",
        "                max_depth -= 1\r\n",
        "\r\n",
        "            if n_features is None:\r\n",
        "                n_features = X.shape[1]\r\n",
        "\r\n",
        "            column, value, gain = self._find_best_split(X, y, n_features)\r\n",
        "            assert gain is not None\r\n",
        "\r\n",
        "            self.column_index = column\r\n",
        "            self.threshold = value\r\n",
        "            self.impurity = gain\r\n",
        "\r\n",
        "            # Split dataset\r\n",
        "            left_X, right_X, left_target, right_target = split_dataset(X, y, column, value)\r\n",
        "\r\n",
        "            # Grow left and right child\r\n",
        "            self.left_child = Tree_regr(self.criterion)\r\n",
        "            self.left_child.fit(\r\n",
        "                left_X, left_target, n_features, max_depth\r\n",
        "            )\r\n",
        "\r\n",
        "            self.right_child = Tree_regr(self.criterion)\r\n",
        "            self.right_child.fit(\r\n",
        "                right_X, right_target, n_features, max_depth\r\n",
        "            )\r\n",
        "        except AssertionError:\r\n",
        "            self.outcome_probs = np.sum(y) / y.shape[0]\r\n",
        "\r\n",
        "\r\n",
        "    def predict_row(self, row):\r\n",
        "        \"\"\"Predict single row.\"\"\"\r\n",
        "        if not self.is_terminal:\r\n",
        "            if row[self.column_index] < self.threshold:\r\n",
        "                return self.left_child.predict_row(row)\r\n",
        "            else:\r\n",
        "                return self.right_child.predict_row(row)\r\n",
        "        \r\n",
        "        return self.outcome_probs\r\n",
        "\r\n",
        "    def predict(self, X):\r\n",
        "        \"\"\"Make predictions.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            X (numpy-array): The test input samples. 2-dimensional numpy array.\r\n",
        "        \"\"\"  \r\n",
        "        result = np.zeros(X.shape[0])\r\n",
        "        for i in range(X.shape[0]):\r\n",
        "            result[i] = self.predict_row(X[i, :])\r\n",
        "        return result"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j3vFsvwVnJr"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def f(X):\n",
        "    return X[:, 0]**3 + np.log(np.exp(X[:, 1]) + np.exp(X[:, 2])) + np.sqrt(abs(X[:, 3])) * X[:, 4]\n",
        "\n",
        "n_samples = 100\n",
        "\n",
        "stdv = 1. / np.sqrt(5)\n",
        "\n",
        "X = np.random.uniform(-stdv, stdv, size = (n_samples, 5))\n",
        "\n",
        "y = f(X)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edKnZl9jVnJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a385f6b-ee22-4318-cb79-9c5b2d8a3952"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\r\n",
        "\r\n",
        "for i in range(1, 30):\r\n",
        "  model = Tree_regr()\r\n",
        "  model.fit(X_train, y_train, max_depth = i)\r\n",
        "  y_pred = model.predict(X_test)\r\n",
        "  model = DecisionTreeRegressor(max_depth=i)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  y_pred1 = model.predict(X_test)\r\n",
        "  print(f\"Max_depth is {i} error custom is: {mean_squared_error(y_test, y_pred)} and error library is: {mean_squared_error(y_test, y_pred1)}\")"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max_depth is 1 error custom is: 0.042132433424130755 and error library is: 0.04655772182457095\n",
            "Max_depth is 2 error custom is: 0.041847632764265875 and error library is: 0.02924484064395915\n",
            "Max_depth is 3 error custom is: 0.036105867567797205 and error library is: 0.024317545202471876\n",
            "Max_depth is 4 error custom is: 0.04951686679956828 and error library is: 0.015382953128865545\n",
            "Max_depth is 5 error custom is: 0.034844511150769746 and error library is: 0.018605428489511112\n",
            "Max_depth is 6 error custom is: 0.041662439829512884 and error library is: 0.013225891714559786\n",
            "Max_depth is 7 error custom is: 0.0392605686698559 and error library is: 0.016746543839924796\n",
            "Max_depth is 8 error custom is: 0.041656362001188926 and error library is: 0.01395449998417152\n",
            "Max_depth is 9 error custom is: 0.04161274561904176 and error library is: 0.01286070297775704\n",
            "Max_depth is 10 error custom is: 0.0471094429823863 and error library is: 0.015222040973847436\n",
            "Max_depth is 11 error custom is: 0.05481240338746415 and error library is: 0.015317302821033663\n",
            "Max_depth is 12 error custom is: 0.042251998754875175 and error library is: 0.011253925993212689\n",
            "Max_depth is 13 error custom is: 0.05288647397874471 and error library is: 0.013586886890843372\n",
            "Max_depth is 14 error custom is: 0.050739072682799334 and error library is: 0.012256589339792543\n",
            "Max_depth is 15 error custom is: 0.05252584782696927 and error library is: 0.014581204864382224\n",
            "Max_depth is 16 error custom is: 0.05621339183853933 and error library is: 0.016450048360551955\n",
            "Max_depth is 17 error custom is: 0.0589959402780733 and error library is: 0.01558194875344795\n",
            "Max_depth is 18 error custom is: 0.05670424753880247 and error library is: 0.015400097116546121\n",
            "Max_depth is 19 error custom is: 0.05802872097512465 and error library is: 0.016549052507808047\n",
            "Max_depth is 20 error custom is: 0.05302477655051885 and error library is: 0.013117325049853443\n",
            "Max_depth is 21 error custom is: 0.06626360103387137 and error library is: 0.014060038913030898\n",
            "Max_depth is 22 error custom is: 0.05276926738858358 and error library is: 0.01656718544339987\n",
            "Max_depth is 23 error custom is: 0.0603440043705533 and error library is: 0.014738762449513726\n",
            "Max_depth is 24 error custom is: 0.07344670461507227 and error library is: 0.014455739190810588\n",
            "Max_depth is 25 error custom is: 0.051932747046695225 and error library is: 0.013869518848221244\n",
            "Max_depth is 26 error custom is: 0.0647074169039501 and error library is: 0.016061322250838\n",
            "Max_depth is 27 error custom is: 0.07835835986563595 and error library is: 0.01594328007940784\n",
            "Max_depth is 28 error custom is: 0.06619640788644583 and error library is: 0.016744374723001113\n",
            "Max_depth is 29 error custom is: 0.0710473629497226 and error library is: 0.014088399882133127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtd_WrWyVnJr"
      },
      "source": [
        "**Task 8 (2 points)**. Write your own _RandomForestRegressor_ class with MSE split criterion and settable parameter _max_depth_.  Demonstrate its consistency on the proposed artificial data (or some other) by comparing MSE of train predictions with [sklearn.ensemble.RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html).<br>\n",
        "\n",
        "Provide examples for different max_depth parameter.<br>\n",
        "\n",
        "**! You are allowed to use only NumPy library** in this assigment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrKt9BJbVnJr"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xAZN24uAZEc"
      },
      "source": [
        "class RandomForestREGR(object):\r\n",
        "    \"\"\"\r\n",
        "    A random forest classifier.\r\n",
        "    A random forest is a meta estimator that fits a number of decision tree\r\n",
        "    classifiers on various sub-samples of the dataset and uses averaging to\r\n",
        "    improve the predictive accuracy and control overfitting.\r\n",
        "    \r\n",
        "    Args:\r\n",
        "        n_estimators : int, default=10\r\n",
        "            The number of trees in the forest.\r\n",
        "\r\n",
        "        max_depth : int, default=None\r\n",
        "            The maximum depth of the tree. If None, then nodes are expanded until\r\n",
        "            all leaves are pure.        \r\n",
        "\r\n",
        "        n_features : int, default=None\r\n",
        "            The number of features to consider when looking for the best split.\r\n",
        "            If None, then `n_features=sqrt(n_features)`.\r\n",
        "\r\n",
        "        criterion : {\"gini\", \"entropy\"}, default=\"gini\"\r\n",
        "            The function to measure the quality of a split. Supported criteria are\r\n",
        "            \"gini\" for the Gini impurity and \"entropy\" for the information gain.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, n_estimators=10, max_depth=None, n_features=None, criterion=\"entropy\", bootstrap=False):\r\n",
        "        self.n_estimators = n_estimators\r\n",
        "        self.max_depth = max_depth\r\n",
        "        self.n_features = n_features\r\n",
        "        self.bootstrap = bootstrap\r\n",
        "        \r\n",
        "        self.trees = [Tree_regr() for _ in range(n_estimators)]\r\n",
        "        \r\n",
        "    def _init_data(self, X, y):\r\n",
        "        \"\"\"Ensure data are in the expected format.\r\n",
        "        Ensures X and y are stored as numpy ndarrays by converting from an\r\n",
        "        array-like object if necessary. \r\n",
        "        Parameters\r\n",
        "        Args:\r\n",
        "            X : array-like\r\n",
        "                Feature dataset.\r\n",
        "            y : array-like, default=None\r\n",
        "                Target values. By default is required, but if y_required = false\r\n",
        "                then may be omitted.\r\n",
        "        \"\"\"\r\n",
        "        self.size = len(X)\r\n",
        "        \r\n",
        "        if not isinstance(X, np.ndarray):\r\n",
        "            self.X = np.array(X)\r\n",
        "        else:\r\n",
        "            self.X = X\r\n",
        "\r\n",
        "        if not isinstance(y, np.ndarray):\r\n",
        "            self.y = np.array(y)\r\n",
        "        else:\r\n",
        "            self.y = y\r\n",
        "            \r\n",
        "    def bootstrap_data(self, size):\r\n",
        "        return np.random.randint(size, size=size)\r\n",
        "    \r\n",
        "    def fit(self, X, y):\r\n",
        "        \"\"\"Fit model.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            X (numpy-array): The training input samples. 2-dimensional numpy array.\r\n",
        "            y (numpy-array): The target values. 1-dimensional numpy array.\r\n",
        "        \"\"\"         \r\n",
        "        if self.n_features is None:\r\n",
        "            self.n_features = int(np.sqrt(X.shape[1]))\r\n",
        "        elif X.shape[1] < self.n_features:\r\n",
        "            raise ValueError(f\"'n_features should be <= n_features'\")\r\n",
        "            \r\n",
        "        self._init_data(X, y)\r\n",
        "        \r\n",
        "        for tree in self.trees:\r\n",
        "            \r\n",
        "                \r\n",
        "            tree.fit(\r\n",
        "                X,\r\n",
        "                y,\r\n",
        "                n_features=self.n_features,\r\n",
        "                max_depth=self.max_depth,\r\n",
        "            )\r\n",
        "            \r\n",
        "    def predict(self, X):\r\n",
        "        \"\"\"Make predictions.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            X (numpy-array): The test data input samples. 2-dimensional numpy array.\r\n",
        "        \"\"\"            \r\n",
        "        if not isinstance(X, np.ndarray):\r\n",
        "            X = np.array(X)\r\n",
        "\r\n",
        "        if self.X is not None:\r\n",
        "            predictions = np.zeros(len(X))\r\n",
        "            for i in range(len(X)):\r\n",
        "                row_pred = 0.\r\n",
        "\r\n",
        "                for tree in self.trees:\r\n",
        "                    row_pred += tree.predict_row(X[i, :])\r\n",
        "                    \r\n",
        "                \r\n",
        "                row_pred /= self.n_estimators\r\n",
        "                predictions[i] = row_pred\r\n",
        "                \r\n",
        "            \r\n",
        "                \r\n",
        "            return predictions  \r\n",
        "        else:\r\n",
        "            raise ValueError(\"You should fit a model before `predict`\")"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLaRzu13BDZy",
        "outputId": "958fa24f-6b01-4e47-8a45-a641754a2da1"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\r\n",
        "for i in range(1, 30):\r\n",
        "  model = RandomForestREGR(max_depth= i)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  y_pred = model.predict(X_test)\r\n",
        "  model = RandomForestRegressor(max_depth= i)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  y_pred1 = model.predict(X_test)\r\n",
        "  print(f\"Max_depth is {i} error custom is: {mean_squared_error(y_test, y_pred)} and error library is: {mean_squared_error(y_test, y_pred1)}\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max_depth is 1 error custom is: 0.04085475478714749 and error library is: 0.033346216631858855\n",
            "Max_depth is 2 error custom is: 0.038623747792972826 and error library is: 0.019485056334719163\n",
            "Max_depth is 3 error custom is: 0.037868633016257314 and error library is: 0.012648778065480499\n",
            "Max_depth is 4 error custom is: 0.040899431967081526 and error library is: 0.01140680535317258\n",
            "Max_depth is 5 error custom is: 0.03929836977203931 and error library is: 0.010355848284143905\n",
            "Max_depth is 6 error custom is: 0.03811245779783354 and error library is: 0.009986683806360853\n",
            "Max_depth is 7 error custom is: 0.04028648810390393 and error library is: 0.00928412618812936\n",
            "Max_depth is 8 error custom is: 0.04037394476584546 and error library is: 0.010608958960269947\n",
            "Max_depth is 9 error custom is: 0.04215489609419189 and error library is: 0.008894618005845598\n",
            "Max_depth is 10 error custom is: 0.0405987812612347 and error library is: 0.009649279933149811\n",
            "Max_depth is 11 error custom is: 0.04452179754743403 and error library is: 0.008160647046398788\n",
            "Max_depth is 12 error custom is: 0.04470530364550957 and error library is: 0.009333310503430255\n",
            "Max_depth is 13 error custom is: 0.05020007420421726 and error library is: 0.009792457643586643\n",
            "Max_depth is 14 error custom is: 0.0460008709090708 and error library is: 0.00853495140949231\n",
            "Max_depth is 15 error custom is: 0.05221467680888494 and error library is: 0.009290462896516484\n",
            "Max_depth is 16 error custom is: 0.049321089715402784 and error library is: 0.007401235152488438\n",
            "Max_depth is 17 error custom is: 0.04887990315391939 and error library is: 0.009408281411295025\n",
            "Max_depth is 18 error custom is: 0.049764725377189685 and error library is: 0.008747545923064443\n",
            "Max_depth is 19 error custom is: 0.052506070837686526 and error library is: 0.010588224677931544\n",
            "Max_depth is 20 error custom is: 0.04840702318427275 and error library is: 0.00779055445408298\n",
            "Max_depth is 21 error custom is: 0.050206531003486335 and error library is: 0.009079486534679597\n",
            "Max_depth is 22 error custom is: 0.0524775761975636 and error library is: 0.008479541266022832\n",
            "Max_depth is 23 error custom is: 0.054678379423941416 and error library is: 0.00749497938009077\n",
            "Max_depth is 24 error custom is: 0.05427611506593094 and error library is: 0.008033132836410179\n",
            "Max_depth is 25 error custom is: 0.05808589752034392 and error library is: 0.0086283214054305\n",
            "Max_depth is 26 error custom is: 0.06292747467312271 and error library is: 0.0071066888411146\n",
            "Max_depth is 27 error custom is: 0.055298968867294615 and error library is: 0.007725019723703139\n",
            "Max_depth is 28 error custom is: 0.06548544623399874 and error library is: 0.010440819368114827\n",
            "Max_depth is 29 error custom is: 0.06672236451652971 and error library is: 0.009788904016146478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0syc-YyTVnJs"
      },
      "source": [
        "# Homework part 3 (bonus). Speeding up forest training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBrCJpBwVnJs"
      },
      "source": [
        "**Task 9 (3 points)** Devise a way to speed up training against the default version of our custom _RandomForestClassifier_ or your own _RandomForestRegressor_. You may want use [`joblib`](https://joblib.readthedocs.io/en/latest/) for parallelizing trees training. Provide graphs of time dependences on the number of trees in your _fast_ version with different number of cores / threads used against default one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne4La3r9FBsE"
      },
      "source": [
        "У каждого есть класса есть параметр njobs, отвечающий за количество выделенных потоков => joblib не нужен"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rExEdHCtVnJs"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "9XHwtto7GI4a",
        "outputId": "a40b48f2-ca29-4adb-ec81-c912c6985e3d"
      },
      "source": [
        "import time\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "X = df.drop('target', axis = 1)\r\n",
        "y = df['target']\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2000, random_state = 2021)\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 2021)\r\n",
        "x, er = [], []\r\n",
        "for i in range(1, 17):\r\n",
        "  start_time = time.time()\r\n",
        "  model = RandomForestClassifier(random_state= 2021, n_estimators=50, criterion='entropy', min_samples_leaf=1, n_jobs = i)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  y_pred = model.predict(X_val)\r\n",
        "  x.append(i)\r\n",
        "  er.append(time.time() - start_time)\r\n",
        "plt.plot(x, er)\r\n",
        "plt.title('time dependence core')\r\n",
        "plt.show()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "SWQi-ZvpKBJp",
        "outputId": "9a970a77-5a09-4ca8-8533-9d498f043a83"
      },
      "source": [
        "x, er = [], []\r\n",
        "for i in range(1, 100):\r\n",
        "  start_time = time.time()\r\n",
        "  model = RandomForestClassifier(random_state= 2021, n_estimators=i, criterion='entropy', min_samples_leaf=1)\r\n",
        "  model.fit(X_train, y_train)\r\n",
        "  y_pred = model.predict(X_val)\r\n",
        "  x.append(i)\r\n",
        "  er.append(time.time() - start_time)\r\n",
        "plt.scatter(x, er)\r\n",
        "plt.title('time dependence number_trees')\r\n",
        "plt.show()"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ONcD3OKjiv"
      },
      "source": [
        "Графики могут варьироваться от вашего железа, в любой момент можете подкрутить второй график взависимости от производительности вашей системы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJNvgu9jJlvJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}